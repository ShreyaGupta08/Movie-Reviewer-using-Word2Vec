{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "labeledData = pd.read_csv(\"labeledTrainData.tsv\", header = 0, delimiter='\\t', quoting= 3)\n",
    "unlabeledData = pd.read_csv(\"unlabeledTrainData.tsv\", header = 0, delimiter='\\t', quoting = 3)\n",
    "testData =  pd.read_csv(\"testData.tsv\", header = 0, delimiter='\\t', quoting = 3)\n",
    "\n",
    "labeledData.shape, unlabeledData.shape, testData.shape\n",
    "\n",
    "# transform raw review\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# unlike bag of words, Word2Vec needs a list of words so we return the same\n",
    "# instead of returning a string as done previously.\n",
    "def transformSentence(rawReview, remove_stopwords = False):\n",
    "    #remove punctuation marks\n",
    "    noHTML = bs(rawReview, \"lxml\").get_text()\n",
    "    \n",
    "    #remove punctuation marks\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", noHTML)\n",
    "    \n",
    "    #convert to lower case and split\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    #optional removing stopwords\n",
    "    if remove_stopwords:\n",
    "        sw = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if w not in sw]\n",
    "    \n",
    "    #return list of words in the review\n",
    "    return words\n",
    "\n",
    "clean_review = transformSentence(unlabeledData.review[0])\n",
    "# clean_review\n",
    "\n",
    "unlabeledData.review[0]\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "raw_sentence=  tokenizer.tokenize(unlabeledData.review[0].strip())\n",
    "raw_sentence\n",
    "\n",
    "def transformReview(rawReview, tokenizer, remove_stopwords = False):\n",
    "    \n",
    "    #convert paragraph into sentences\n",
    "    raw_sentence = tokenizer.tokenize(rawReview.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    #for each sentence, convert it into list of words\n",
    "    for sentence in raw_sentence:\n",
    "        if(len(sentence) > 0):\n",
    "            sentences.append(transformSentence(sentence, remove_stopwords))\n",
    "    \n",
    "    #return list of sentences each broken into words i.e. a list of lists\n",
    "    return sentences\n",
    "\n",
    "s = transformReview(unlabeledData.review[0], tokenizer)\n",
    "print(len(s))\n",
    "print(len(s[0]))\n",
    "s\n",
    "\n",
    "# form a 3D matrix of all sentences from all reviews\n",
    "sentences = []\n",
    "\n",
    "for review in unlabeledData.review:\n",
    "    sentences += transformReview(review, tokenizer)\n",
    "\n",
    "for review in labeledData.review:\n",
    "    sentences += transformReview(review, tokenizer)\n",
    "\n",
    "len(sentences[0])\n",
    "\n",
    "sentences[0]\n",
    "\n",
    "## Model Training\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "nFeatures = 300\n",
    "minWordCount = 40\n",
    "nWorkers = 4\n",
    "contextWindow = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers = nWorkers, \\\n",
    "                         size = nFeatures, min_count = minWordCount, \\\n",
    "                         window = contextWindow, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "\n",
    "# Identifying the words most dissimilar\n",
    "print(model.wv.doesnt_match(\"man woman shot kitchen\".split()))\n",
    "print(model.wv.doesnt_match(\"man woman child kitten\".split()))\n",
    "\n",
    "# identifying differences in meaning\n",
    "print(model.wv.doesnt_match(\"france england europe berlin\".split()))\n",
    "\n",
    "# imperfections\n",
    "print(model.wv.doesnt_match(\"france england america berlin \".split()))\n",
    "\n",
    "# finding most similar \n",
    "model.wv.most_similar(\"man\")\n",
    "\n",
    "# one pretty surprising thing occured when you find words similar to woman- results-what a wow.\n",
    "model.wv.most_similar(\"woman\")\n",
    "\n",
    "model.wv.most_similar(\"queen\")\n",
    "\n",
    "model.wv.most_similar(\"awful\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
