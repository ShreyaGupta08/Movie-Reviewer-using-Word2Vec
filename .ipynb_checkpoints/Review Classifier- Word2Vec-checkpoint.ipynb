{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeledData = pd.read_csv(\"labeledTrainData.tsv\", header = 0, delimiter='\\t', quoting= 3)\n",
    "unlabeledData = pd.read_csv(\"unlabeledTrainData.tsv\", header = 0, delimiter='\\t', quoting = 3)\n",
    "testData =  pd.read_csv(\"testData.tsv\", header = 0, delimiter='\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 3), (50000, 2), (25000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledData.shape, unlabeledData.shape, testData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform raw review\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unlike bag of words, Word2Vec needs a list of words so we return the same\n",
    "# instead of returning a string as done previously.\n",
    "def transformSentence(rawReview, remove_stopwords = False):\n",
    "    #remove punctuation marks\n",
    "    noHTML = bs(rawReview, \"lxml\").get_text()\n",
    "    \n",
    "    #remove punctuation marks\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", noHTML)\n",
    "    \n",
    "    #convert to lower case and split\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    #optional removing stopwords\n",
    "    if remove_stopwords:\n",
    "        sw = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if w not in sw]\n",
    "    \n",
    "    #return list of words in the review\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_review = transformSentence(unlabeledData.review[0])\n",
    "# clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\\\"Hey, let\\'s pool our money together and make a really bad movie!\\\\\" Or something like that. What ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. All corners were cut, except the one that would have prevented this film\\'s release. Life\\'s like that.\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeledData.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Watching Time Chasers, it obvious that it was made by a bunch of friends.',\n",
       " 'Maybe they were sitting around one day in film school and said, \\\\\"Hey, let\\'s pool our money together and make a really bad movie!\\\\\" Or something like that.',\n",
       " 'What ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc.',\n",
       " \"All corners were cut, except the one that would have prevented this film's release.\",\n",
       " 'Life\\'s like that.\"']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentence=  tokenizer.tokenize(unlabeledData.review[0].strip())\n",
    "raw_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformReview(rawReview, tokenizer, remove_stopwords = False):\n",
    "    \n",
    "    #convert paragraph into sentences\n",
    "    raw_sentence = tokenizer.tokenize(rawReview.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    #for each sentence, convert it into list of words\n",
    "    for sentence in raw_sentence:\n",
    "        if(len(sentence) > 0):\n",
    "            sentences.append(transformSentence(sentence, remove_stopwords))\n",
    "    \n",
    "    #return list of sentences each broken into words i.e. a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['watching',\n",
       "  'time',\n",
       "  'chasers',\n",
       "  'it',\n",
       "  'obvious',\n",
       "  'that',\n",
       "  'it',\n",
       "  'was',\n",
       "  'made',\n",
       "  'by',\n",
       "  'a',\n",
       "  'bunch',\n",
       "  'of',\n",
       "  'friends'],\n",
       " ['maybe',\n",
       "  'they',\n",
       "  'were',\n",
       "  'sitting',\n",
       "  'around',\n",
       "  'one',\n",
       "  'day',\n",
       "  'in',\n",
       "  'film',\n",
       "  'school',\n",
       "  'and',\n",
       "  'said',\n",
       "  'hey',\n",
       "  'let',\n",
       "  's',\n",
       "  'pool',\n",
       "  'our',\n",
       "  'money',\n",
       "  'together',\n",
       "  'and',\n",
       "  'make',\n",
       "  'a',\n",
       "  'really',\n",
       "  'bad',\n",
       "  'movie',\n",
       "  'or',\n",
       "  'something',\n",
       "  'like',\n",
       "  'that'],\n",
       " ['what',\n",
       "  'ever',\n",
       "  'they',\n",
       "  'said',\n",
       "  'they',\n",
       "  'still',\n",
       "  'ended',\n",
       "  'up',\n",
       "  'making',\n",
       "  'a',\n",
       "  'really',\n",
       "  'bad',\n",
       "  'movie',\n",
       "  'dull',\n",
       "  'story',\n",
       "  'bad',\n",
       "  'script',\n",
       "  'lame',\n",
       "  'acting',\n",
       "  'poor',\n",
       "  'cinematography',\n",
       "  'bottom',\n",
       "  'of',\n",
       "  'the',\n",
       "  'barrel',\n",
       "  'stock',\n",
       "  'music',\n",
       "  'etc'],\n",
       " ['all',\n",
       "  'corners',\n",
       "  'were',\n",
       "  'cut',\n",
       "  'except',\n",
       "  'the',\n",
       "  'one',\n",
       "  'that',\n",
       "  'would',\n",
       "  'have',\n",
       "  'prevented',\n",
       "  'this',\n",
       "  'film',\n",
       "  's',\n",
       "  'release'],\n",
       " ['life', 's', 'like', 'that']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = transformReview(unlabeledData.review[0], tokenizer)\n",
    "print(len(s))\n",
    "print(len(s[0]))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/shreya/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/shreya/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/shreya/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/shreya/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/shreya/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/shreya/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/shreya/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# form a 3D matrix of all sentences from all reviews\n",
    "sentences = []\n",
    "\n",
    "for review in unlabeledData.review:\n",
    "    sentences += transformReview(review, tokenizer)\n",
    "\n",
    "for review in labeledData.review:\n",
    "    sentences += transformReview(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watching',\n",
       " 'time',\n",
       " 'chasers',\n",
       " 'it',\n",
       " 'obvious',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'made',\n",
       " 'by',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'friends']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nFeatures = 300\n",
    "minWordCount = 40\n",
    "nWorkers = 4\n",
    "contextWindow = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-07 11:54:47,142 : INFO : collecting all words and their counts\n",
      "2019-01-07 11:54:47,143 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-07 11:54:47,225 : INFO : PROGRESS: at sentence #10000, processed 229703 words, keeping 17646 word types\n",
      "2019-01-07 11:54:47,295 : INFO : PROGRESS: at sentence #20000, processed 453167 words, keeping 25105 word types\n",
      "2019-01-07 11:54:47,380 : INFO : PROGRESS: at sentence #30000, processed 680392 words, keeping 30426 word types\n",
      "2019-01-07 11:54:47,453 : INFO : PROGRESS: at sentence #40000, processed 906246 words, keeping 34664 word types\n",
      "2019-01-07 11:54:47,527 : INFO : PROGRESS: at sentence #50000, processed 1129037 words, keeping 38303 word types\n",
      "2019-01-07 11:54:47,589 : INFO : PROGRESS: at sentence #60000, processed 1357442 words, keeping 41636 word types\n",
      "2019-01-07 11:54:47,651 : INFO : PROGRESS: at sentence #70000, processed 1585964 words, keeping 44634 word types\n",
      "2019-01-07 11:54:47,714 : INFO : PROGRESS: at sentence #80000, processed 1813003 words, keeping 47209 word types\n",
      "2019-01-07 11:54:47,778 : INFO : PROGRESS: at sentence #90000, processed 2032234 words, keeping 49506 word types\n",
      "2019-01-07 11:54:47,844 : INFO : PROGRESS: at sentence #100000, processed 2261098 words, keeping 51744 word types\n",
      "2019-01-07 11:54:47,905 : INFO : PROGRESS: at sentence #110000, processed 2488045 words, keeping 53936 word types\n",
      "2019-01-07 11:54:47,980 : INFO : PROGRESS: at sentence #120000, processed 2716349 words, keeping 56103 word types\n",
      "2019-01-07 11:54:48,053 : INFO : PROGRESS: at sentence #130000, processed 2944492 words, keeping 57944 word types\n",
      "2019-01-07 11:54:48,120 : INFO : PROGRESS: at sentence #140000, processed 3165941 words, keeping 59731 word types\n",
      "2019-01-07 11:54:48,197 : INFO : PROGRESS: at sentence #150000, processed 3390004 words, keeping 61468 word types\n",
      "2019-01-07 11:54:48,272 : INFO : PROGRESS: at sentence #160000, processed 3614060 words, keeping 63157 word types\n",
      "2019-01-07 11:54:48,351 : INFO : PROGRESS: at sentence #170000, processed 3844792 words, keeping 64804 word types\n",
      "2019-01-07 11:54:48,423 : INFO : PROGRESS: at sentence #180000, processed 4070169 words, keeping 66423 word types\n",
      "2019-01-07 11:54:48,503 : INFO : PROGRESS: at sentence #190000, processed 4300448 words, keeping 68049 word types\n",
      "2019-01-07 11:54:48,581 : INFO : PROGRESS: at sentence #200000, processed 4528919 words, keeping 69490 word types\n",
      "2019-01-07 11:54:48,689 : INFO : PROGRESS: at sentence #210000, processed 4760314 words, keeping 70944 word types\n",
      "2019-01-07 11:54:48,803 : INFO : PROGRESS: at sentence #220000, processed 4984715 words, keeping 72457 word types\n",
      "2019-01-07 11:54:48,910 : INFO : PROGRESS: at sentence #230000, processed 5207413 words, keeping 73743 word types\n",
      "2019-01-07 11:54:49,028 : INFO : PROGRESS: at sentence #240000, processed 5431569 words, keeping 75076 word types\n",
      "2019-01-07 11:54:49,158 : INFO : PROGRESS: at sentence #250000, processed 5658767 words, keeping 76392 word types\n",
      "2019-01-07 11:54:49,285 : INFO : PROGRESS: at sentence #260000, processed 5886067 words, keeping 77608 word types\n",
      "2019-01-07 11:54:49,394 : INFO : PROGRESS: at sentence #270000, processed 6110756 words, keeping 78835 word types\n",
      "2019-01-07 11:54:49,486 : INFO : PROGRESS: at sentence #280000, processed 6330730 words, keeping 80042 word types\n",
      "2019-01-07 11:54:49,561 : INFO : PROGRESS: at sentence #290000, processed 6559062 words, keeping 81248 word types\n",
      "2019-01-07 11:54:49,628 : INFO : PROGRESS: at sentence #300000, processed 6785564 words, keeping 82334 word types\n",
      "2019-01-07 11:54:49,698 : INFO : PROGRESS: at sentence #310000, processed 7014237 words, keeping 83475 word types\n",
      "2019-01-07 11:54:49,797 : INFO : PROGRESS: at sentence #320000, processed 7236718 words, keeping 84580 word types\n",
      "2019-01-07 11:54:49,917 : INFO : PROGRESS: at sentence #330000, processed 7462750 words, keeping 85659 word types\n",
      "2019-01-07 11:54:50,034 : INFO : PROGRESS: at sentence #340000, processed 7683874 words, keeping 86788 word types\n",
      "2019-01-07 11:54:50,153 : INFO : PROGRESS: at sentence #350000, processed 7911736 words, keeping 87853 word types\n",
      "2019-01-07 11:54:50,269 : INFO : PROGRESS: at sentence #360000, processed 8135596 words, keeping 88841 word types\n",
      "2019-01-07 11:54:50,382 : INFO : PROGRESS: at sentence #370000, processed 8360509 words, keeping 89879 word types\n",
      "2019-01-07 11:54:50,485 : INFO : PROGRESS: at sentence #380000, processed 8584811 words, keeping 90952 word types\n",
      "2019-01-07 11:54:50,580 : INFO : PROGRESS: at sentence #390000, processed 8809702 words, keeping 91964 word types\n",
      "2019-01-07 11:54:50,670 : INFO : PROGRESS: at sentence #400000, processed 9035721 words, keeping 92889 word types\n",
      "2019-01-07 11:54:50,786 : INFO : PROGRESS: at sentence #410000, processed 9263851 words, keeping 93862 word types\n",
      "2019-01-07 11:54:50,897 : INFO : PROGRESS: at sentence #420000, processed 9486853 words, keeping 94742 word types\n",
      "2019-01-07 11:54:50,989 : INFO : PROGRESS: at sentence #430000, processed 9712380 words, keeping 95723 word types\n",
      "2019-01-07 11:54:51,056 : INFO : PROGRESS: at sentence #440000, processed 9940566 words, keeping 96687 word types\n",
      "2019-01-07 11:54:51,127 : INFO : PROGRESS: at sentence #450000, processed 10169542 words, keeping 97534 word types\n",
      "2019-01-07 11:54:51,201 : INFO : PROGRESS: at sentence #460000, processed 10392708 words, keeping 98412 word types\n",
      "2019-01-07 11:54:51,277 : INFO : PROGRESS: at sentence #470000, processed 10615466 words, keeping 99271 word types\n",
      "2019-01-07 11:54:51,420 : INFO : PROGRESS: at sentence #480000, processed 10839420 words, keeping 100113 word types\n",
      "2019-01-07 11:54:51,550 : INFO : PROGRESS: at sentence #490000, processed 11060772 words, keeping 100962 word types\n",
      "2019-01-07 11:54:51,671 : INFO : PROGRESS: at sentence #500000, processed 11283493 words, keeping 101814 word types\n",
      "2019-01-07 11:54:51,791 : INFO : PROGRESS: at sentence #510000, processed 11513918 words, keeping 102754 word types\n",
      "2019-01-07 11:54:51,919 : INFO : PROGRESS: at sentence #520000, processed 11743007 words, keeping 103581 word types\n",
      "2019-01-07 11:54:52,014 : INFO : PROGRESS: at sentence #530000, processed 11968404 words, keeping 104503 word types\n",
      "2019-01-07 11:54:52,082 : INFO : PROGRESS: at sentence #540000, processed 12196284 words, keeping 105558 word types\n",
      "2019-01-07 11:54:52,144 : INFO : PROGRESS: at sentence #550000, processed 12422679 words, keeping 106671 word types\n",
      "2019-01-07 11:54:52,242 : INFO : PROGRESS: at sentence #560000, processed 12643636 words, keeping 107769 word types\n",
      "2019-01-07 11:54:52,328 : INFO : PROGRESS: at sentence #570000, processed 12873296 words, keeping 108797 word types\n",
      "2019-01-07 11:54:52,402 : INFO : PROGRESS: at sentence #580000, processed 13093150 words, keeping 109738 word types\n",
      "2019-01-07 11:54:52,475 : INFO : PROGRESS: at sentence #590000, processed 13314470 words, keeping 110631 word types\n",
      "2019-01-07 11:54:52,553 : INFO : PROGRESS: at sentence #600000, processed 13539727 words, keeping 111518 word types\n",
      "2019-01-07 11:54:52,620 : INFO : PROGRESS: at sentence #610000, processed 13761121 words, keeping 112326 word types\n",
      "2019-01-07 11:54:52,699 : INFO : PROGRESS: at sentence #620000, processed 13985256 words, keeping 113240 word types\n",
      "2019-01-07 11:54:52,771 : INFO : PROGRESS: at sentence #630000, processed 14208190 words, keeping 114018 word types\n",
      "2019-01-07 11:54:52,845 : INFO : PROGRESS: at sentence #640000, processed 14429706 words, keeping 114801 word types\n",
      "2019-01-07 11:54:52,921 : INFO : PROGRESS: at sentence #650000, processed 14654417 words, keeping 115689 word types\n",
      "2019-01-07 11:54:52,993 : INFO : PROGRESS: at sentence #660000, processed 14873002 words, keeping 116407 word types\n",
      "2019-01-07 11:54:53,071 : INFO : PROGRESS: at sentence #670000, processed 15095962 words, keeping 117165 word types\n",
      "2019-01-07 11:54:53,145 : INFO : PROGRESS: at sentence #680000, processed 15320772 words, keeping 117991 word types\n",
      "2019-01-07 11:54:53,221 : INFO : PROGRESS: at sentence #690000, processed 15545090 words, keeping 118754 word types\n",
      "2019-01-07 11:54:53,295 : INFO : PROGRESS: at sentence #700000, processed 15768274 words, keeping 119435 word types\n",
      "2019-01-07 11:54:53,374 : INFO : PROGRESS: at sentence #710000, processed 15990985 words, keeping 120164 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-07 11:54:53,444 : INFO : PROGRESS: at sentence #720000, processed 16218387 words, keeping 120845 word types\n",
      "2019-01-07 11:54:53,515 : INFO : PROGRESS: at sentence #730000, processed 16443252 words, keeping 121505 word types\n",
      "2019-01-07 11:54:53,591 : INFO : PROGRESS: at sentence #740000, processed 16668236 words, keeping 122283 word types\n",
      "2019-01-07 11:54:53,657 : INFO : PROGRESS: at sentence #750000, processed 16893906 words, keeping 123008 word types\n",
      "2019-01-07 11:54:53,731 : INFO : PROGRESS: at sentence #760000, processed 17117338 words, keeping 123702 word types\n",
      "2019-01-07 11:54:53,810 : INFO : PROGRESS: at sentence #770000, processed 17343232 words, keeping 124376 word types\n",
      "2019-01-07 11:54:53,868 : INFO : PROGRESS: at sentence #780000, processed 17557571 words, keeping 125099 word types\n",
      "2019-01-07 11:54:53,935 : INFO : PROGRESS: at sentence #790000, processed 17781366 words, keeping 125806 word types\n",
      "2019-01-07 11:54:53,966 : INFO : collected 126186 word types from a corpus of 17901685 raw words and 795538 sentences\n",
      "2019-01-07 11:54:53,967 : INFO : Loading a fresh vocabulary\n",
      "2019-01-07 11:54:54,061 : INFO : effective_min_count=40 retains 16731 unique words (13% of original 126186, drops 109455)\n",
      "2019-01-07 11:54:54,062 : INFO : effective_min_count=40 leaves 17335523 word corpus (96% of original 17901685, drops 566162)\n",
      "2019-01-07 11:54:54,144 : INFO : deleting the raw counts dictionary of 126186 items\n",
      "2019-01-07 11:54:54,149 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-01-07 11:54:54,150 : INFO : downsampling leaves estimated 12862720 word corpus (74.2% of prior 17335523)\n",
      "2019-01-07 11:54:54,217 : INFO : estimated required memory for 16731 words and 300 dimensions: 48519900 bytes\n",
      "2019-01-07 11:54:54,218 : INFO : resetting layer weights\n",
      "2019-01-07 11:54:54,482 : INFO : training model with 4 workers on 16731 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-07 11:54:55,510 : INFO : EPOCH 1 - PROGRESS: at 4.04% examples, 517384 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:54:56,516 : INFO : EPOCH 1 - PROGRESS: at 8.37% examples, 536917 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:54:57,520 : INFO : EPOCH 1 - PROGRESS: at 12.64% examples, 541253 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:54:58,528 : INFO : EPOCH 1 - PROGRESS: at 16.92% examples, 542906 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:54:59,536 : INFO : EPOCH 1 - PROGRESS: at 21.19% examples, 543795 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-07 11:55:00,549 : INFO : EPOCH 1 - PROGRESS: at 25.47% examples, 545022 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:01,553 : INFO : EPOCH 1 - PROGRESS: at 29.22% examples, 535598 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:02,563 : INFO : EPOCH 1 - PROGRESS: at 33.43% examples, 536066 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:03,571 : INFO : EPOCH 1 - PROGRESS: at 37.48% examples, 534227 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-07 11:55:04,571 : INFO : EPOCH 1 - PROGRESS: at 41.50% examples, 532428 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:05,588 : INFO : EPOCH 1 - PROGRESS: at 45.41% examples, 528930 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:06,588 : INFO : EPOCH 1 - PROGRESS: at 49.48% examples, 528420 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:07,597 : INFO : EPOCH 1 - PROGRESS: at 53.60% examples, 528249 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:08,627 : INFO : EPOCH 1 - PROGRESS: at 57.65% examples, 526824 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:09,629 : INFO : EPOCH 1 - PROGRESS: at 62.06% examples, 528878 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:10,638 : INFO : EPOCH 1 - PROGRESS: at 66.34% examples, 530486 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:11,647 : INFO : EPOCH 1 - PROGRESS: at 70.64% examples, 531480 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:12,662 : INFO : EPOCH 1 - PROGRESS: at 75.01% examples, 532528 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:13,665 : INFO : EPOCH 1 - PROGRESS: at 79.35% examples, 533509 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:14,682 : INFO : EPOCH 1 - PROGRESS: at 83.75% examples, 534351 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-07 11:55:15,682 : INFO : EPOCH 1 - PROGRESS: at 88.06% examples, 535170 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:16,699 : INFO : EPOCH 1 - PROGRESS: at 92.42% examples, 535833 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:17,706 : INFO : EPOCH 1 - PROGRESS: at 96.75% examples, 536665 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:18,402 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-07 11:55:18,421 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-07 11:55:18,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-07 11:55:18,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-07 11:55:18,432 : INFO : EPOCH - 1 : training on 17901685 raw words (12860286 effective words) took 23.9s, 537328 effective words/s\n",
      "2019-01-07 11:55:19,458 : INFO : EPOCH 2 - PROGRESS: at 3.81% examples, 486714 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:20,473 : INFO : EPOCH 2 - PROGRESS: at 7.94% examples, 505236 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:21,495 : INFO : EPOCH 2 - PROGRESS: at 12.04% examples, 509935 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-07 11:55:22,496 : INFO : EPOCH 2 - PROGRESS: at 16.17% examples, 516833 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:23,501 : INFO : EPOCH 2 - PROGRESS: at 20.48% examples, 523238 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:24,503 : INFO : EPOCH 2 - PROGRESS: at 24.81% examples, 530088 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:25,512 : INFO : EPOCH 2 - PROGRESS: at 29.10% examples, 532594 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:26,522 : INFO : EPOCH 2 - PROGRESS: at 33.37% examples, 534395 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:27,529 : INFO : EPOCH 2 - PROGRESS: at 37.66% examples, 536025 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:28,552 : INFO : EPOCH 2 - PROGRESS: at 42.06% examples, 537873 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:29,560 : INFO : EPOCH 2 - PROGRESS: at 46.42% examples, 539335 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:30,568 : INFO : EPOCH 2 - PROGRESS: at 50.60% examples, 538898 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:31,570 : INFO : EPOCH 2 - PROGRESS: at 54.75% examples, 538689 words/s, in_qsize 8, out_qsize 1\n",
      "2019-01-07 11:55:32,572 : INFO : EPOCH 2 - PROGRESS: at 58.83% examples, 537541 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:33,582 : INFO : EPOCH 2 - PROGRESS: at 63.10% examples, 537684 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:34,594 : INFO : EPOCH 2 - PROGRESS: at 67.39% examples, 538615 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:35,608 : INFO : EPOCH 2 - PROGRESS: at 71.23% examples, 535644 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:36,619 : INFO : EPOCH 2 - PROGRESS: at 75.57% examples, 536200 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:37,627 : INFO : EPOCH 2 - PROGRESS: at 79.96% examples, 537221 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:38,638 : INFO : EPOCH 2 - PROGRESS: at 84.36% examples, 538016 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:39,649 : INFO : EPOCH 2 - PROGRESS: at 88.74% examples, 538748 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:40,653 : INFO : EPOCH 2 - PROGRESS: at 92.97% examples, 538891 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-07 11:55:41,654 : INFO : EPOCH 2 - PROGRESS: at 97.34% examples, 539704 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:42,220 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-07 11:55:42,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-07 11:55:42,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-07 11:55:42,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-07 11:55:42,237 : INFO : EPOCH - 2 : training on 17901685 raw words (12861597 effective words) took 23.8s, 540491 effective words/s\n",
      "2019-01-07 11:55:43,277 : INFO : EPOCH 3 - PROGRESS: at 4.26% examples, 542175 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-07 11:55:44,296 : INFO : EPOCH 3 - PROGRESS: at 8.59% examples, 545520 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:45,306 : INFO : EPOCH 3 - PROGRESS: at 12.93% examples, 548269 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:46,310 : INFO : EPOCH 3 - PROGRESS: at 16.80% examples, 536508 words/s, in_qsize 7, out_qsize 1\n",
      "2019-01-07 11:55:47,315 : INFO : EPOCH 3 - PROGRESS: at 21.03% examples, 537510 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:48,320 : INFO : EPOCH 3 - PROGRESS: at 24.76% examples, 528767 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:49,322 : INFO : EPOCH 3 - PROGRESS: at 28.75% examples, 526930 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:50,346 : INFO : EPOCH 3 - PROGRESS: at 33.10% examples, 529377 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-07 11:55:51,352 : INFO : EPOCH 3 - PROGRESS: at 37.38% examples, 531540 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:52,365 : INFO : EPOCH 3 - PROGRESS: at 41.61% examples, 532195 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:53,371 : INFO : EPOCH 3 - PROGRESS: at 45.79% examples, 532458 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:54,382 : INFO : EPOCH 3 - PROGRESS: at 49.60% examples, 528281 words/s, in_qsize 7, out_qsize 2\n",
      "2019-01-07 11:55:55,397 : INFO : EPOCH 3 - PROGRESS: at 53.78% examples, 528387 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:56,397 : INFO : EPOCH 3 - PROGRESS: at 57.43% examples, 524497 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:57,402 : INFO : EPOCH 3 - PROGRESS: at 61.59% examples, 524772 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:55:58,404 : INFO : EPOCH 3 - PROGRESS: at 65.46% examples, 523284 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-07 11:55:59,406 : INFO : EPOCH 3 - PROGRESS: at 69.22% examples, 521134 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:00,437 : INFO : EPOCH 3 - PROGRESS: at 73.37% examples, 520724 words/s, in_qsize 8, out_qsize 1\n",
      "2019-01-07 11:56:01,444 : INFO : EPOCH 3 - PROGRESS: at 77.47% examples, 520700 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:02,453 : INFO : EPOCH 3 - PROGRESS: at 81.54% examples, 520252 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:03,466 : INFO : EPOCH 3 - PROGRESS: at 85.88% examples, 521441 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:04,475 : INFO : EPOCH 3 - PROGRESS: at 89.85% examples, 520645 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-07 11:56:05,476 : INFO : EPOCH 3 - PROGRESS: at 93.81% examples, 520109 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:06,492 : INFO : EPOCH 3 - PROGRESS: at 97.85% examples, 519614 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:06,975 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-07 11:56:06,991 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-07 11:56:06,994 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-07 11:56:06,998 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-07 11:56:06,999 : INFO : EPOCH - 3 : training on 17901685 raw words (12861226 effective words) took 24.7s, 519826 effective words/s\n",
      "2019-01-07 11:56:08,025 : INFO : EPOCH 4 - PROGRESS: at 3.93% examples, 500796 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:09,031 : INFO : EPOCH 4 - PROGRESS: at 8.21% examples, 525150 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:10,045 : INFO : EPOCH 4 - PROGRESS: at 12.54% examples, 534038 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:11,050 : INFO : EPOCH 4 - PROGRESS: at 16.80% examples, 538110 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:12,056 : INFO : EPOCH 4 - PROGRESS: at 20.59% examples, 527420 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:13,069 : INFO : EPOCH 4 - PROGRESS: at 23.68% examples, 505462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:14,093 : INFO : EPOCH 4 - PROGRESS: at 27.74% examples, 507299 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:15,098 : INFO : EPOCH 4 - PROGRESS: at 30.93% examples, 494881 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:16,101 : INFO : EPOCH 4 - PROGRESS: at 35.12% examples, 499465 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:17,102 : INFO : EPOCH 4 - PROGRESS: at 39.27% examples, 503229 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:18,110 : INFO : EPOCH 4 - PROGRESS: at 43.45% examples, 505992 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:19,112 : INFO : EPOCH 4 - PROGRESS: at 47.71% examples, 509143 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:20,133 : INFO : EPOCH 4 - PROGRESS: at 51.93% examples, 511041 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:21,139 : INFO : EPOCH 4 - PROGRESS: at 55.72% examples, 509193 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:22,144 : INFO : EPOCH 4 - PROGRESS: at 58.83% examples, 501919 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-07 11:56:23,154 : INFO : EPOCH 4 - PROGRESS: at 62.99% examples, 503400 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:24,159 : INFO : EPOCH 4 - PROGRESS: at 67.17% examples, 505709 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:25,178 : INFO : EPOCH 4 - PROGRESS: at 71.40% examples, 507364 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:26,178 : INFO : EPOCH 4 - PROGRESS: at 75.74% examples, 509688 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:27,185 : INFO : EPOCH 4 - PROGRESS: at 79.96% examples, 510963 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:28,197 : INFO : EPOCH 4 - PROGRESS: at 84.31% examples, 512624 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:29,197 : INFO : EPOCH 4 - PROGRESS: at 88.51% examples, 513762 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:30,216 : INFO : EPOCH 4 - PROGRESS: at 92.91% examples, 515587 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:31,222 : INFO : EPOCH 4 - PROGRESS: at 97.34% examples, 517523 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:31,796 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-07 11:56:31,803 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-07 11:56:31,805 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-07 11:56:31,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-07 11:56:31,812 : INFO : EPOCH - 4 : training on 17901685 raw words (12864879 effective words) took 24.8s, 518695 effective words/s\n",
      "2019-01-07 11:56:32,841 : INFO : EPOCH 5 - PROGRESS: at 4.15% examples, 528369 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:33,843 : INFO : EPOCH 5 - PROGRESS: at 8.26% examples, 528924 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:34,862 : INFO : EPOCH 5 - PROGRESS: at 12.49% examples, 530919 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:35,871 : INFO : EPOCH 5 - PROGRESS: at 16.57% examples, 529792 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:36,880 : INFO : EPOCH 5 - PROGRESS: at 20.80% examples, 531692 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:37,885 : INFO : EPOCH 5 - PROGRESS: at 24.32% examples, 519252 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:38,886 : INFO : EPOCH 5 - PROGRESS: at 28.63% examples, 524863 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:39,909 : INFO : EPOCH 5 - PROGRESS: at 32.32% examples, 517009 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-07 11:56:40,939 : INFO : EPOCH 5 - PROGRESS: at 36.16% examples, 512983 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:41,946 : INFO : EPOCH 5 - PROGRESS: at 39.39% examples, 503070 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-07 11:56:42,981 : INFO : EPOCH 5 - PROGRESS: at 42.80% examples, 495609 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-07 11:56:43,990 : INFO : EPOCH 5 - PROGRESS: at 46.53% examples, 494033 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:44,999 : INFO : EPOCH 5 - PROGRESS: at 50.60% examples, 495949 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:46,001 : INFO : EPOCH 5 - PROGRESS: at 54.86% examples, 499869 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:47,009 : INFO : EPOCH 5 - PROGRESS: at 59.23% examples, 503553 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:48,022 : INFO : EPOCH 5 - PROGRESS: at 63.37% examples, 504799 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:49,026 : INFO : EPOCH 5 - PROGRESS: at 67.44% examples, 506177 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-07 11:56:50,045 : INFO : EPOCH 5 - PROGRESS: at 71.45% examples, 506202 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:51,050 : INFO : EPOCH 5 - PROGRESS: at 75.57% examples, 506929 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:52,067 : INFO : EPOCH 5 - PROGRESS: at 79.62% examples, 506998 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:53,069 : INFO : EPOCH 5 - PROGRESS: at 83.81% examples, 508095 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:54,079 : INFO : EPOCH 5 - PROGRESS: at 87.61% examples, 506931 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:55,082 : INFO : EPOCH 5 - PROGRESS: at 91.02% examples, 503864 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:56,083 : INFO : EPOCH 5 - PROGRESS: at 94.08% examples, 499317 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:57,094 : INFO : EPOCH 5 - PROGRESS: at 97.57% examples, 496923 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-07 11:56:57,798 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-07 11:56:57,836 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-07 11:56:57,840 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-07 11:56:57,852 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-07 11:56:57,855 : INFO : EPOCH - 5 : training on 17901685 raw words (12862747 effective words) took 26.0s, 494118 effective words/s\n",
      "2019-01-07 11:56:57,858 : INFO : training on a 89508425 raw words (64310735 effective words) took 123.4s, 521263 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers = nWorkers, \\\n",
    "                         size = nFeatures, min_count = minWordCount, \\\n",
    "                         window = contextWindow, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-07 11:56:57,868 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-07 11:57:06,875 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-01-07 11:57:06,877 : INFO : not storing attribute vectors_norm\n",
      "2019-01-07 11:57:06,880 : INFO : not storing attribute cum_table\n",
      "2019-01-07 11:57:10,045 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot\n",
      "kitten\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# Identifying the words most dissimilar\n",
    "print(model.wv.doesnt_match(\"man woman shot kitchen\".split()))\n",
    "print(model.wv.doesnt_match(\"man woman child kitten\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berlin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# identifying differences in meaning\n",
    "print(model.wv.doesnt_match(\"france england europe berlin\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "america\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# imperfections\n",
    "print(model.wv.doesnt_match(\"france england america berlin \".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6073155403137207),\n",
       " ('lady', 0.577538013458252),\n",
       " ('lad', 0.5705820322036743),\n",
       " ('monk', 0.5522242784500122),\n",
       " ('guy', 0.5355662107467651),\n",
       " ('soldier', 0.5269594192504883),\n",
       " ('farmer', 0.526430606842041),\n",
       " ('men', 0.5187892317771912),\n",
       " ('person', 0.5143908858299255),\n",
       " ('doctor', 0.501837968826294)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding most similar \n",
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('prostitute', 0.687353253364563),\n",
       " ('lady', 0.6743903756141663),\n",
       " ('widow', 0.6555472612380981),\n",
       " ('girl', 0.6531828045845032),\n",
       " ('man', 0.6073155403137207),\n",
       " ('nun', 0.6046781539916992),\n",
       " ('housewife', 0.5961974859237671),\n",
       " ('waitress', 0.5844756364822388),\n",
       " ('whore', 0.5788934230804443),\n",
       " ('nurse', 0.567044734954834)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one pretty surprising thing occured when you find words similar to woman- results-what a wow.\n",
    "model.wv.most_similar(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6527175307273865),\n",
       " ('latifah', 0.6238234043121338),\n",
       " ('eva', 0.6182420253753662),\n",
       " ('maid', 0.6065552830696106),\n",
       " ('regina', 0.6056563854217529),\n",
       " ('bride', 0.6001290082931519),\n",
       " ('nun', 0.5974670052528381),\n",
       " ('goddess', 0.5938385725021362),\n",
       " ('belle', 0.5892914533615112),\n",
       " ('mistress', 0.586790919303894)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7815027832984924),\n",
       " ('atrocious', 0.7411219477653503),\n",
       " ('horrible', 0.7312250137329102),\n",
       " ('dreadful', 0.7201114892959595),\n",
       " ('abysmal', 0.7110464572906494),\n",
       " ('appalling', 0.6754916906356812),\n",
       " ('horrendous', 0.6701131463050842),\n",
       " ('horrid', 0.6584107875823975),\n",
       " ('lousy', 0.6349300146102905),\n",
       " ('amateurish', 0.6325801610946655)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
